{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf817f42-4dfd-404e-9512-53632246be2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 15 19:29:14 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             67W /  700W |       1MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a980429-0602-4eb5-8a26-cbde34e56a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unsloth in /usr/local/lib/python3.10/dist-packages (2024.11.7)\n",
      "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.12.1)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.0.28.post3)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.45.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (24.2)\n",
      "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2.5.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (2.1.3)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.26.2)\n",
      "Requirement already satisfied: transformers>=4.46.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.46.2)\n",
      "Requirement already satisfied: unsloth-zoo>=2024.11.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2024.11.5)\n",
      "Requirement already satisfied: tyro in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.8.14)\n",
      "Requirement already satisfied: hf-transfer in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.1.8)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.44.1)\n",
      "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.1.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.67.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.2.0)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.1.0)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.20.3)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.1.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (6.1.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (0.4.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (18.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.16.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.2.3)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2024.9.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.11.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unsloth) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.3.1.170)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.4.5.8)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.1.4)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth) (0.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth) (2024.11.6)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (13.9.4)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (1.7.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (24.2.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.17.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.2.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (6.1.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb) (59.6.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.18.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install unsloth\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "498e0410-30fb-45a6-aa7a-659b506bd34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%capture\n",
    "# # This cell will take time\n",
    "# !pip install unsloth\n",
    "# # Also get the latest nightly Unsloth!\n",
    "# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08f4883c-ed01-43bc-a22e-cfd84d79b24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "985a486d-39df-40c4-89be-413817083901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c2a35f-bc94-423b-b1ad-6a8251aa4154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.209 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 9.0. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7538492-a863-493d-af1c-025bed57708d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.11.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ddf38b1-bcaa-4943-80b7-017fc15f9e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'is_correct', 'answer', 'solution'],\n",
       "        num_rows: 1000000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'is_correct', 'answer', 'solution'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download and load competition dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\")\n",
    "# print and see dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b411b12-4ddd-4e0c-bfff-f5cf67e50b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are an expert math teacher and problem solver. Your task is to carefully review a math question, the provided answer, and the detailed solution steps.\n",
    "Verify if the provided answer is correct based strictly on the solution steps.\n",
    "Do not include any explanations or additional text, and do not provide anything other than the words 'True' or 'False'.\n",
    "### Math Question:\n",
    "{}\n",
    "\n",
    "### Provided Answer:\n",
    "{}\n",
    "\n",
    "### Detailed Solution Steps:\n",
    "{}\n",
    "\n",
    "### Correctness:\n",
    "(True or False)\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    question = examples[\"question\"]\n",
    "    ans = examples[\"answer\"]\n",
    "    solution = examples[\"solution\"]\n",
    "    output = examples[\"is_correct\"]\n",
    "    texts = []\n",
    "    for ques, ans_text, sol, correct in zip(question, ans, solution, output):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = prompt.format(ques, ans_text, sol, correct) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3010a5cb-9691-4467-afc7-28d4f16b318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the formatting function to the sampled dataset\n",
    "train_dataset = dataset['train'].map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bd87866-937f-4b84-8e7d-e1ca7684a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import random\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Define the number of samples for training and evaluation\n",
    "train_size_per_class = 20000  # 15,000 'True' and 15,000 'False' for training (total 30,000)\n",
    "eval_size_per_class = 4000    # 1,500 'True' and 1,500 'False' for evaluation (total 3,000)\n",
    "\n",
    "# Filter the dataset into 'True' and 'False' samples\n",
    "true_samples = train_dataset.filter(lambda example: example['is_correct'] == True)\n",
    "false_samples = train_dataset.filter(lambda example: example['is_correct'] == False)\n",
    "\n",
    "# Ensure we have enough samples of each class\n",
    "assert len(true_samples) >= train_size_per_class + eval_size_per_class, \"Not enough 'True' samples\"\n",
    "assert len(false_samples) >= train_size_per_class + eval_size_per_class, \"Not enough 'False' samples\"\n",
    "\n",
    "# Shuffle and sample the training data\n",
    "true_train_sampled = true_samples.shuffle(seed=3407).select(range(train_size_per_class))\n",
    "false_train_sampled = false_samples.shuffle(seed=3407).select(range(train_size_per_class))\n",
    "\n",
    "# Shuffle and sample the evaluation data\n",
    "true_eval_sampled = true_samples.shuffle(seed=3407).select(range(train_size_per_class, train_size_per_class + eval_size_per_class))\n",
    "false_eval_sampled = false_samples.shuffle(seed=3407).select(range(train_size_per_class, train_size_per_class + eval_size_per_class))\n",
    "\n",
    "balanced_train_dataset = concatenate_datasets([true_train_sampled, false_train_sampled]).shuffle(seed=3407)\n",
    "balanced_eval_dataset = concatenate_datasets([true_eval_sampled, false_eval_sampled]).shuffle(seed=3407)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e6e6e90-7328-4ab6-92f3-502e7a3c8552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Train Dataset Distribution:\n",
      "is_correct\n",
      "True     20000\n",
      "False    20000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Balanced Eval Dataset Distribution:\n",
      "is_correct\n",
      "False    4000\n",
      "True     4000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert the datasets to Pandas DataFrames\n",
    "train_distribution_df = balanced_train_dataset.to_pandas()\n",
    "eval_distribution_df = balanced_eval_dataset.to_pandas()\n",
    "\n",
    "# Print the distribution of the 'is_correct' column\n",
    "train_distribution = train_distribution_df['is_correct'].value_counts()\n",
    "eval_distribution = eval_distribution_df['is_correct'].value_counts()\n",
    "\n",
    "print(\"Balanced Train Dataset Distribution:\")\n",
    "print(train_distribution)\n",
    "\n",
    "print(\"\\nBalanced Eval Dataset Distribution:\")\n",
    "print(eval_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c4c18f5-8ecc-4b82-bf81-d8fb30945e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d804d43c-0cc8-405a-a964-a7d8c99ef169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:             x86_64\n",
      "  CPU op-mode(s):         32-bit, 64-bit\n",
      "  Address sizes:          52 bits physical, 57 bits virtual\n",
      "  Byte Order:             Little Endian\n",
      "CPU(s):                   64\n",
      "  On-line CPU(s) list:    0-63\n",
      "Vendor ID:                AuthenticAMD\n",
      "  Model name:             AMD EPYC 9124 16-Core Processor\n",
      "    CPU family:           25\n",
      "    Model:                17\n",
      "    Thread(s) per core:   2\n",
      "    Core(s) per socket:   16\n",
      "    Socket(s):            2\n",
      "    Stepping:             1\n",
      "    Frequency boost:      enabled\n",
      "    CPU max MHz:          3711.9141\n",
      "    CPU min MHz:          1500.0000\n",
      "    BogoMIPS:             5990.97\n",
      "    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge m\n",
      "                          ca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall\n",
      "                           nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep\n",
      "                          _good nopl nonstop_tsc cpuid extd_apicid aperfmperf ra\n",
      "                          pl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 ss\n",
      "                          e4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lah\n",
      "                          f_lm cmp_legacy svm extapic cr8_legacy abm sse4a misal\n",
      "                          ignsse 3dnowprefetch osvw ibs skinit wdt tce topoext p\n",
      "                          erfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb ca\n",
      "                          t_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibp\n",
      "                          b stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep \n",
      "                          bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed ad\n",
      "                          x smap avx512ifma clflushopt clwb avx512cd sha_ni avx5\n",
      "                          12bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc c\n",
      "                          qm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 c\n",
      "                          lzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc a\n",
      "                          rat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean f\n",
      "                          lushbyasid decodeassists pausefilter pfthreshold avic \n",
      "                          v_vmsave_vmload vgif v_spec_ctrl avx512vbmi umip pku o\n",
      "                          spke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx\n",
      "                          512_bitalg avx512_vpopcntdq la57 rdpid overflow_recov \n",
      "                          succor smca fsrm flush_l1d\n",
      "Virtualization features:  \n",
      "  Virtualization:         AMD-V\n",
      "Caches (sum of all):      \n",
      "  L1d:                    1 MiB (32 instances)\n",
      "  L1i:                    1 MiB (32 instances)\n",
      "  L2:                     32 MiB (32 instances)\n",
      "  L3:                     128 MiB (8 instances)\n",
      "NUMA:                     \n",
      "  NUMA node(s):           2\n",
      "  NUMA node0 CPU(s):      0-15,32-47\n",
      "  NUMA node1 CPU(s):      16-31,48-63\n",
      "Vulnerabilities:          \n",
      "  Gather data sampling:   Not affected\n",
      "  Itlb multihit:          Not affected\n",
      "  L1tf:                   Not affected\n",
      "  Mds:                    Not affected\n",
      "  Meltdown:               Not affected\n",
      "  Mmio stale data:        Not affected\n",
      "  Reg file data sampling: Not affected\n",
      "  Retbleed:               Not affected\n",
      "  Spec rstack overflow:   Mitigation; safe RET\n",
      "  Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prct\n",
      "                          l and seccomp\n",
      "  Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointe\n",
      "                          r sanitization\n",
      "  Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditiona\n",
      "                          l; STIBP always-on; RSB filling; PBRSB-eIBRS Not affec\n",
      "                          ted; BHI Not affected\n",
      "  Srbds:                  Not affected\n",
      "  Tsx async abort:        Not affected\n"
     ]
    }
   ],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "717e17d5-88b7-47f7-939d-0a8ea39df9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "import random\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=32, # smaller for better perf?  # higher batch size?\n",
    "    gradient_accumulation_steps=8, # 8 \n",
    "    # warmup_steps=50,    # 30\n",
    "    warmup_ratio=0.1,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4, #1e-4 might be too large for fine tun   # 2e-4\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    optim=\"adamw_hf\",    # adamw_torch\n",
    "    weight_decay=0.01,  # 0.03 too high  # 0.01\n",
    "    lr_scheduler_type=\"cosine\",    # cosine_with_restarts\n",
    "    seed=3407,\n",
    "    output_dir=\"outputs\",\n",
    "    dataloader_num_workers=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    logging_dir=\"./logs\",  # Directory to save TensorBoard logs\n",
    "    logging_steps=1,\n",
    "    # report_to=\"tensorboard\",  # Enable TensorBoard logging\n",
    "    report_to=\"wandb\",\n",
    "    dataloader_pin_memory=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,    # Set to False since lower loss is better\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    max_grad_norm=0.5,               # Add gradient clipping  # try 0.5\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=balanced_train_dataset,\n",
    "    eval_dataset=balanced_eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=32,\n",
    "    packing=False,  # Can make training 5x faster for short sequences\n",
    "    args=training_args,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69a9c85f-2cff-40e7-8e80-136ddf28e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64168f45-e2cd-48da-8338-bd15b3df7588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "!wandb login 06e028f429168761bc1fcee0499e25c9e432267e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "776af029-b822-4f09-a62a-038319d02501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbietabigbit\u001b[0m (\u001b[33mbietabigbit-new-york-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20241115_193005-zrmfstvz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bietabigbit-new-york-university/DL-Fall-24%20Kaggle%20Contest/runs/zrmfstvz' target=\"_blank\">final run large lora i do not give a fuck2</a></strong> to <a href='https://wandb.ai/bietabigbit-new-york-university/DL-Fall-24%20Kaggle%20Contest' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bietabigbit-new-york-university/DL-Fall-24%20Kaggle%20Contest' target=\"_blank\">https://wandb.ai/bietabigbit-new-york-university/DL-Fall-24%20Kaggle%20Contest</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bietabigbit-new-york-university/DL-Fall-24%20Kaggle%20Contest/runs/zrmfstvz' target=\"_blank\">https://wandb.ai/bietabigbit-new-york-university/DL-Fall-24%20Kaggle%20Contest/runs/zrmfstvz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/bietabigbit-new-york-university/DL-Fall-24%20Kaggle%20Contest/runs/zrmfstvz?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f145c5b3700>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Initialize a new W&B run\n",
    "wandb.init(\n",
    "    project=\"DL-Fall-24 Kaggle Contest\",   # Set your project name\n",
    "    name=\"final run large lora i do not give a fuck2\",          # Optional: Set a custom run name\n",
    "    config=training_args,          # Pass in the training arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaee8c9c-d636-465a-a1cb-faac53a0ad65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 40,000 | Num Epochs = 2\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 256 | Total steps = 312\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='312' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [312/312 3:16:09, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.104900</td>\n",
       "      <td>1.002021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.678700</td>\n",
       "      <td>0.663405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.627100</td>\n",
       "      <td>0.620605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.611300</td>\n",
       "      <td>0.604267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.609900</td>\n",
       "      <td>0.593916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.579100</td>\n",
       "      <td>0.586251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.603300</td>\n",
       "      <td>0.579356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.566900</td>\n",
       "      <td>0.573634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.605400</td>\n",
       "      <td>0.567766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.574700</td>\n",
       "      <td>0.561682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.544800</td>\n",
       "      <td>0.556200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.548000</td>\n",
       "      <td>0.550128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.544300</td>\n",
       "      <td>0.544440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.524900</td>\n",
       "      <td>0.538674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.523500</td>\n",
       "      <td>0.533096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.483200</td>\n",
       "      <td>0.528372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.492400</td>\n",
       "      <td>0.524009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.483700</td>\n",
       "      <td>0.518896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.507500</td>\n",
       "      <td>0.514136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.465800</td>\n",
       "      <td>0.509045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.469400</td>\n",
       "      <td>0.504796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.462200</td>\n",
       "      <td>0.501080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.470700</td>\n",
       "      <td>0.497245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.473800</td>\n",
       "      <td>0.494019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.436200</td>\n",
       "      <td>0.491396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.486800</td>\n",
       "      <td>0.489050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.465900</td>\n",
       "      <td>0.487402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.487300</td>\n",
       "      <td>0.486325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.465300</td>\n",
       "      <td>0.485571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.453700</td>\n",
       "      <td>0.485300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.485203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=312, training_loss=0.5611275769770145, metrics={'train_runtime': 11812.4217, 'train_samples_per_second': 6.773, 'train_steps_per_second': 0.026, 'total_flos': 2.6785260773509693e+18, 'train_loss': 0.5611275769770145, 'epoch': 1.9968})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1447b1a7-457c-4756-9503-11283489b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "# model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "452c3ea2-2e2f-4245-9969-51cb7cb26519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 40,000 | Num Epochs = 2\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 256 | Total steps = 312\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='312' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [312/312 3:16:12, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.454800</td>\n",
       "      <td>0.483406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.470100</td>\n",
       "      <td>0.482860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.465400</td>\n",
       "      <td>0.486612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.459800</td>\n",
       "      <td>0.490479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.461800</td>\n",
       "      <td>0.488419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.438900</td>\n",
       "      <td>0.484585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.463100</td>\n",
       "      <td>0.480610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.475399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.472700</td>\n",
       "      <td>0.469415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.442200</td>\n",
       "      <td>0.465048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.412200</td>\n",
       "      <td>0.459429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.424100</td>\n",
       "      <td>0.454104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.449786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.410700</td>\n",
       "      <td>0.443612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.407900</td>\n",
       "      <td>0.439090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.341700</td>\n",
       "      <td>0.441689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.352300</td>\n",
       "      <td>0.437933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.349200</td>\n",
       "      <td>0.435059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.369900</td>\n",
       "      <td>0.430517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.345100</td>\n",
       "      <td>0.428606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.347400</td>\n",
       "      <td>0.425597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.423067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.357600</td>\n",
       "      <td>0.420771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.354800</td>\n",
       "      <td>0.418370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.328800</td>\n",
       "      <td>0.416802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.374200</td>\n",
       "      <td>0.415438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.355600</td>\n",
       "      <td>0.413903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.379000</td>\n",
       "      <td>0.413250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>0.412671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.342700</td>\n",
       "      <td>0.412576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.346700</td>\n",
       "      <td>0.412532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=312, training_loss=0.3984998298379091, metrics={'train_runtime': 11795.0911, 'train_samples_per_second': 6.782, 'train_steps_per_second': 0.026, 'total_flos': 2.6785260773509693e+18, 'train_loss': 0.3984998298379091, 'epoch': 1.9968})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a938ee3b-6821-4f7d-9652-5d0cdc59c64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 40,000 | Num Epochs = 2\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 256 | Total steps = 312\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='85' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 85/312 51:33 < 2:21:01, 0.03 it/s, Epoch 0.54/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.351900</td>\n",
       "      <td>0.411528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.357700</td>\n",
       "      <td>0.412967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.350600</td>\n",
       "      <td>0.417545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.344600</td>\n",
       "      <td>0.424376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.355200</td>\n",
       "      <td>0.430502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.340800</td>\n",
       "      <td>0.433188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>0.432162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.354600</td>\n",
       "      <td>0.431295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.366300</td>\n",
       "      <td>0.431390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:157\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m<string>:380\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:64\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:2241\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d218a082-2abc-47d5-a283-32a94208c2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "628c9823-1d61-4073-83af-5321096fc273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.43139007687568665\n"
     ]
    }
   ],
   "source": [
    "eval_metrics = trainer.evaluate()\n",
    "print(f\"Validation Loss: {eval_metrics['eval_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "336fd070-7e11-407a-bdc4-b16ab90a7805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def generate_kaggle_predictions(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    max_new_tokens=64\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate predictions for Kaggle submission and calculate accuracy.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model\n",
    "        tokenizer: The tokenizer\n",
    "        test_dataset: Test dataset containing questions and answers\n",
    "        batch_size: Batch size for inference\n",
    "        max_new_tokens: Maximum number of new tokens to generate\n",
    "\n",
    "    Returns:\n",
    "        list: List of boolean predictions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = test_dataset['is_correct']  # Ground truth labels\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # Process test data in batches\n",
    "    for i in tqdm(range(0, len(test_dataset), batch_size), desc=\"Generating predictions\"):\n",
    "        batch_questions = test_dataset['question'][i:i + batch_size]\n",
    "        batch_answers = test_dataset['answer'][i:i + batch_size]\n",
    "        # batch_solutions = test_dataset.get('solution', [\"\"] * len(batch_questions))\n",
    "        batch_solutions = test_dataset['solution'][i:i + batch_size] if 'solution' in test_dataset.column_names else [\"\"] * len(batch_questions)\n",
    "        batch_labels = all_labels[i:i + batch_size]\n",
    "\n",
    "        # Prepare prompts for each question-answer pair\n",
    "        prompts = []\n",
    "        for question, answer, solution in zip(batch_questions, batch_answers, batch_solutions):\n",
    "            prompt = f\"\"\"\n",
    "You are an expert math teacher and problem solver. Your task is to carefully review a math question, the provided answer, and the detailed solution steps.\n",
    "Verify if the provided answer is correct based strictly on the solution steps.\n",
    "Do not include any explanations or additional text, and do not provide anything other than the words 'True' or 'False'.\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Given Answer:\n",
    "{answer}\n",
    "\n",
    "### Given Solution:\n",
    "{solution}\n",
    "\n",
    "\n",
    "### Correctness:\n",
    "(True or False)\n",
    "\"\"\"\n",
    "            prompts.append(prompt)\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            prompts,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        # Generate predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            # Process each output in the batch\n",
    "            for j, output in enumerate(outputs):\n",
    "                # Get only the generated part (exclude input)\n",
    "                input_length = inputs.input_ids[j].shape[0]\n",
    "                prediction = tokenizer.decode(output[input_length:], skip_special_tokens=True)\n",
    "                # Convert to boolean based on presence of \"true\" in prediction\n",
    "                bool_pred = \"true\" in prediction.strip().lower()\n",
    "\n",
    "                all_predictions.append(bool_pred)\n",
    "\n",
    "                # Compare prediction with ground truth and count correct predictions\n",
    "                if bool_pred == batch_labels[j]:\n",
    "                    correct_predictions += 1\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / len(test_dataset)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "def create_submission_file(predictions, output_file=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    Create Kaggle submission file with 'ID' and 'is_correct' columns.\n",
    "\n",
    "    Args:\n",
    "        predictions: List of boolean predictions\n",
    "        output_file: Name of output CSV file\n",
    "    \"\"\"\n",
    "    # Create DataFrame with 'ID' and 'is_correct' columns\n",
    "    # bool_predictions = [pred.strip().lower() == \"true\" for pred in predictions]\n",
    "    bool_predictions = [pred for pred in predictions]\n",
    "    submission_df = pd.DataFrame({\n",
    "        'ID': list(range(len(predictions))),  # ID column from 0 to len(predictions) - 1\n",
    "        'is_correct': bool_predictions\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    print(f\"Submission file saved to {output_file}\")\n",
    "\n",
    "    # Print distribution of predictions\n",
    "    true_count = sum(predictions)\n",
    "    false_count = len(predictions) - true_count\n",
    "    print(f\"\\nPrediction distribution:\")\n",
    "    print(f\"True: {true_count} ({true_count/len(predictions)*100:.2f}%)\")\n",
    "    print(f\"False: {false_count} ({false_count/len(predictions)*100:.2f}%)\")\n",
    "\n",
    "\n",
    "# Main execution function\n",
    "def generate_submission(model, tokenizer, test_dataset, batch_size=8, output_file=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    Generate and save Kaggle submission file.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model\n",
    "        tokenizer: The tokenizer\n",
    "        test_dataset: Test dataset\n",
    "        batch_size: Batch size for inference\n",
    "        output_file: Output file path\n",
    "    \"\"\"\n",
    "    print(\"Generating predictions...\")\n",
    "    predictions = generate_kaggle_predictions(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        test_dataset=test_dataset,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    print(\"\\nCreating submission file...\")\n",
    "    create_submission_file(predictions, output_file)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31a98743-b16d-44ec-9daa-78a52e23e7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f0875f58134121bb4ede591df65687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating predictions:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2939\n",
      "\n",
      "Creating submission file...\n",
      "Submission file saved to submission.csv\n",
      "\n",
      "Prediction distribution:\n",
      "True: 2939 (29.39%)\n",
      "False: 7061 (70.61%)\n"
     ]
    }
   ],
   "source": [
    "# Make sure model is in evaluation mode and using fast inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Generate submission\n",
    "predictions = generate_submission(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    test_dataset=dataset['test'],\n",
    "    batch_size=64,  # Adjust based on your GPU memory\n",
    "    output_file=\"submission.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd912dbe-e1f6-4e91-8764-293eb103399e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6524e3d3-4ec1-43b7-aa29-e4ff31d20e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('outputs/saved_model/tokenizer_config.json',\n",
       " 'outputs/saved_model/special_tokens_map.json',\n",
       " 'outputs/saved_model/tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"outputs/saved_model\")\n",
    "tokenizer.save_pretrained(\"outputs/saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcf73c3f-03e2-4e9d-9a64-a77adbdbe756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"outputs/saved_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"outputs/saved_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a5378-dbd3-4144-a368-ab5c3fb77864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
